{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by: c00k1ez (https://github.com/c00k1ez)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer - is a powerful architecture and it shows state of the art resutls in many seq2seq tasks, like NMT, summarization, and especially for language modeling. Although, the other important feature that transformers also are good at text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quora question pairs classification with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Paraphrase detection is challenging NLP problem of detecting whether multiple phrases have the same meaning. \n",
    " In this notebook, we are going to build a baseline solution for an unusual classification task. \n",
    "\n",
    " For token embeddings we are going to use BERT model. Read more [here](http://jalammar.github.io/illustrated-bert/) and [here](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270).\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from src.bert.models import BertClassifier\n",
    "from src.bert.dataset import PairsDataset\n",
    "from src.bert.data_parser import DataParser\n",
    "from src.utils import seed_all\n",
    "\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'model_name': 'distilbert-base-uncased',\n",
    "    'pad_len': 256,\n",
    "    'batch_size': 30,\n",
    "    'lr': 5e-5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to use BERT without enough computation resources is a distilled version of this model - DistilBert by HuggingFace. \n",
    "[Blogpost](https://medium.com/huggingface/distilbert-8cf3380435b5) about DistilBert and [distillation](https://blog.floydhub.com/knowledge-distillation/).\n",
    "\n",
    "Models, that you can use too:\n",
    "* `BertModel`\n",
    "* `TransfoXLModel`\n",
    "* `XLNetModel`\n",
    "* `ElectraModel`\n",
    "* `RobertaModel`\n",
    "* `XLMRobertaModel`\n",
    "* `AlbertModel`  \n",
    "etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Warning!_ The models will be downloaded from the Internet. Their size could be from 100 Mb to 1-2Gb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(config['model_name'])\n",
    "bert_model = DistilBertModel.from_pretrained(config['model_name'])\n",
    "\n",
    "for p in bert_model.parameters():\n",
    "    p.require_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = DataParser('./data/questions.tsv')\n",
    "train, test = parser.train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Our dataset contains 404290 question pairs.\nThere are 25856 unique tokens in dataset and 11082400 tokens at all.\n\nMost common 10 tokens:\n? : 852404\nthe : 378243\nwhat : 327860\nis : 270750\ni : 224198\nhow : 220949\na : 212794\nto : 206110\nin : 201156\ndo : 161421\n"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print('Our dataset contains {} question pairs.'.format(len(train)))\n",
    "\n",
    "tokens = []\n",
    "for sample in parser.question_pairs:\n",
    "    sample = tokenizer.tokenize(sample[0] + ' ' + sample[1])\n",
    "    tokens.extend(sample)\n",
    "counter = Counter(tokens)\n",
    "print('There are {} unique tokens in dataset and {} tokens at all.'.format(len(counter), sum([v for _,v in dict(counter).items()])))\n",
    "print('Most common 10 tokens:')\n",
    "for token, freq in counter.most_common(10):\n",
    "    print('{} : {}'.format(token, freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PairsDataset(train, tokenizer, config['pad_len'])\n",
    "#test_dataset = PairsDataset(test, tokenizer, config['pad_len'])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, config['batch_size'], shuffle=True)\n",
    "#test_loader = torch.utils.data.DataLoader(test_dataset, config['batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = BertClassifier(bert_model).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, you have to implement `DataParser.train_test_split` and `validation` methods to validate with the macro F1 score.\n",
    "## How to improve this model results?\n",
    "* Play with other models instead of DistilBert: classic BERT, ALBERT, RoBERTa, TinyBERT, etc.\n",
    "* Implement correct MeanPooling/MaxPooling layer (notice that you have `[PAD]` tokens during training and you have to \"exclude\" them from mean or max value computing).\n",
    "* Use more complex model after BERT embeddings.\n",
    "* You can try to use the siamese network to encode the first and second questions independently with metric learning. Read more about it [here](https://parajain.github.io/metric_learning_tutorial/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def validation(model, test_loader, device):\n",
    "    model.eval()\n",
    "    avg_val_loss = []\n",
    "    avg_val_loss_value = -1.0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    ################### INSERT YOUR CODE HERE ###################\n",
    "        \n",
    "    ################### INSERT YOUR CODE HERE ###################\n",
    "    model.train()\n",
    "    return avg_val_loss_value, f1_score(y_true, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, test_loader, optimizer, epoch_num, device, criterion, log_interval=200):\n",
    "    losses = []\n",
    "    avg_loss = []\n",
    "    step = 1\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        for key in batch.keys():\n",
    "            batch[key] = batch[key].to(device)\n",
    "        label = batch['label'].view(-1)\n",
    "        logits = model(batch)\n",
    "        loss = criterion(logits, label)\n",
    "        avg_loss.append(loss.detach().item())\n",
    "        if step % log_interval == 0:\n",
    "            val_loss = sum(avg_loss) / len(avg_loss)\n",
    "            losses.append(val_loss)\n",
    "            avg_loss = []\n",
    "            print('epoch {}\\t[{}/{}]\\ttrain_loss = {:.4f}'.format(epoch_num, step, len(train_loader)))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        step += 1\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "losses = []\n",
    "for epoch in range(EPOCHS):\n",
    "    losses = train_epoch(model, train_loader, None, optimizer, epoch, device, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dialogue generation with GPT2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next task will be try out the text generation abilities of transformers. In this notebook we are going to work with GPT2. This is a model from OpenAI, which showed state of the art results for language modeling in 2019. You can read their original blogpost [here](https://openai.com/blog/better-language-models/).  \n",
    "\n",
    "Let's consider an interesting application of GPT2 model - dialogue generation. Describe this task a bit clearer - we have some context, for example, user question and our model have to generate a relevant answer.  \n",
    "How we can train a model for it? First of all, for input, we need to use special tokens to mark context and model answer, like `[CONTEXT] some context [ANSWER] model answer`. Then there are two possible ways:\n",
    "* train it like classic autoregressive LM,\n",
    "* train it like seq2seq LM. Read more [here](https://arxiv.org/abs/1905.03197).  \n",
    "\n",
    "You can read more about GPT2 [here](http://jalammar.github.io/illustrated-gpt2/) and [here](https://towardsdatascience.com/openai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8).  \n",
    "[Documentation](https://huggingface.co/transformers/model_doc/gpt2.html) for GPT2.\n",
    "\n",
    "| ![seq2seq lm](https://drive.google.com/uc?export=view&id=1NxS-O0Tto2rcFrALhpUBbywriyKlSTL4) |\n",
    "|:--:| \n",
    "| *seq2seq LM* |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import transformers\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config\n",
    "\n",
    "from src.utils import get_answer, seed_all\n",
    "from src.gpt2.data_parser import Dialogue, DataParser\n",
    "from src.gpt2.dataset import DialogueDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_config = {\n",
    "    'pad_len': 100,\n",
    "    'train_batch_size': 10,\n",
    "    'model_name': 'gpt2',\n",
    "    'lr': 5e-5,\n",
    "    'residual_dropout': 0.1,\n",
    "    'embedding_dropout': 0.1,\n",
    "    'attention_dropout': 0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the smallest GPT2 model - it has 124M trainable parameters and requires 500 Mb of disk space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 548M/548M [06:21<00:00, 1.44MB/s]\n"
     ]
    }
   ],
   "source": [
    "config = GPT2Config.from_pretrained(params_config['model_name'])\n",
    "config.resid_pdrop = params_config['residual_dropout']\n",
    "config.attn_pdrop = params_config['attention_dropout']\n",
    "config.embd_pdrop = params_config['embedding_dropout']\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(params_config['model_name'], config=config).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important moment**: we have to add special tokens: `[CONTEXT]` and `[ANSWER]` to tokenizer, then resize model embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few words about tokenizer.\n",
    "GPT2, like some other models, uses [Byte-Pair Encoding](https://leimao.github.io/blog/Byte-Pair-Encoding/) with special tokens in vocabulary.  \n",
    "\n",
    "All tokenizers from `transformers` have unified structure and same methods, so we are going to use a few methods:\n",
    "* `tokenizer.tokenize` to split string unto list of tokens,\n",
    "* `tokenizer.encode` to transform a string into token indexes,\n",
    "* `tokenizer.decode` to transform a list of ids to the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50259, 768)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(params_config['model_name'])\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['[CONTEXT]', '[ANSWER]']})\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can consider our dataset a bit closer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = DataParser('./data/TwitterLowerAsciiCorpus.txt')\n",
    "train, test = parser.train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our dataset contains 8574 context-answer pairs and unique 1983 dialogues\n",
      "There are 10046 unique tokens in dataset and 233751 tokens at all. Notice, that small GPT2 have vocabulary with 50k sub-words.\n",
      "\n",
      "Most common 10 tokens:\n",
      ". : 7196\n",
      "Ġi : 7186\n",
      "Ġthe : 4561\n",
      "Ġto : 4025\n",
      "Ġyou : 3977\n",
      ", : 3602\n",
      "Ġa : 3373\n",
      "Ġit : 3218\n",
      "Ġand : 2602\n",
      "'s : 2285\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print('Our dataset contains {} context-answer pairs and unique {} dialogues'.format(len(parser.all_pairs), len(parser.dialogues)))\n",
    "\n",
    "tokens = []\n",
    "for sample in parser.all_pairs:\n",
    "    sample = tokenizer.tokenize(sample['context'] + ' ' + sample['answer'])\n",
    "    tokens.extend(sample)\n",
    "counter = Counter(tokens)\n",
    "print('There are {} unique tokens in dataset and {} tokens at all. Notice, that small GPT2 have vocabulary with 50k sub-words.'.format(len(counter), sum([v for _,v in dict(counter).items()])))\n",
    "print('')\n",
    "print('Most common 10 tokens:')\n",
    "for token, freq in counter.most_common(10):\n",
    "    print('{} : {}'.format(token, freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can see that we have **really** small dataset for our \"toy\" task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DialogueDataset(train, tokenizer, params_config['pad_len'])\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, params_config['train_batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Another important moment**: we are using AdamW optimizer from `transformers` package, **not** classic Adam and **not** AdamW from `torch.optim`!  \n",
    "[Blogpost](https://www.fast.ai/2018/07/02/adam-weight-decay/) about AdamW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = transformers.AdamW(model.parameters(), lr=params_config['lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you saw earlier, we have a small dataset, so it is quite hard to get a good result and do not overfit.\n",
    "## How to improve this model results?\n",
    "* Implement `train_test_split` method in `DataParser` class and validation loop to calculate [perplexity](https://towardsdatascience.com/perplexity-intuition-and-derivation-105dd481c8f3).\n",
    "* Find optimal `residual_dropout`, `embedding_dropout` and `attention_dropout` probabilities.\n",
    "* Now just a previous sentence is used for training like context for answer. You can rewrite `Dialogue.get_pairs` method to sample one, two, three, or more sentences like context for answer.\n",
    "* You can add a bit more regularizations, for example, throw random tokens from the sample, or swap answer and context with a small probability.\n",
    "* Read about [BPE-dropout](https://arxiv.org/abs/1910.13267). It is hard to implement with `transformers`, so you can just read about this technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, test_loader, device):\n",
    "    ################### INSERT YOUR CODE HERE ###################\n",
    "        \n",
    "    ################### INSERT YOUR CODE HERE ###################\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, test_loader, optimizer, epoch_num, device, log_interval=100):\n",
    "    losses = []\n",
    "    avg_loss = []\n",
    "    step = 1\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids, mask, label = batch['sample'], batch['mask'], batch['label']\n",
    "        input_ids = input_ids.to(device)\n",
    "        mask = mask.to(device)\n",
    "        label = label.to(device)\n",
    "        outputs = model(input_ids, attention_mask=mask, labels=label)\n",
    "        loss, logits = outputs[:2]\n",
    "        avg_loss.append(loss.detach().item())\n",
    "        if step % log_interval == 0:\n",
    "            val_loss = sum(avg_loss) / len(avg_loss)\n",
    "            losses.append(val_loss)\n",
    "            avg_loss = []\n",
    "            print('epoch {}\\t[{}/{}]\\tloss = {:.4f}'.format(epoch_num, step, len(loader), val_loss))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        step += 1\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "losses = []\n",
    "for epoch in range(EPOCHS):\n",
    "    ep_losses = train_epoch(model, train_loader, None, optimizer, epoch, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(torch.device('cpu'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_answer(\"where are you?\", model, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}